{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Solutions to selected problems\n",
    "\n",
    "## 1. Mathematical Functions\n",
    "\n",
    "<ol>\n",
    "<li value=\"2\"><a name=\"01_02\"></a>\n",
    "To prove that $\\exp(x+y) = \\exp(x)\\,\\exp(y)$, we employ the infinite series formula<br/>\n",
    "$$\\exp(x) = \\sum_{n=0}^\\infty\\frac{x^n}{n!}.$$<br/>Here, for notational convenience, we let the sum start from $n = 0$, so that the leading term $1$ in the definition of the exponential is grouped with the rest of the sum as its first term.  This relies on the understanding that $0! \\equiv 1$, and that $x^0 = 1$ (the latter is consistent with the <a href=\"01_mathfunctions.ipynb#powers\">generalized definition of the power operation</a>; but to avoid circular logic, treat this as the *definition* of $x^0$ just for the sake of this proof).  We begin by substituting the series formula into the right-hand side of our target equation:<br/>\n",
    "$$\\exp(x)\\exp(x) = \\left(\\sum_{n=0}^\\infty\\frac{x^n}{n!}\\right)\\;\\left(\\sum_{m=0}^\\infty\\frac{y^m}{m!}\\right).$$<br/>\n",
    "Note that we use the symbol $n$ for the first sum, and the symbol $m$ for the second sum; $n$ and $m$ are <a href=\"https://en.wikipedia.org/wiki/Free_variables_and_bound_variables\">bound variables</a>, whose terms run over the values specified by the summation signs.  The actual choice of symbol used in either sum is unimportant, except that *we must not use the same symbol for both sums*, because the two variables belong to distinct sums.  In other words:<br/>\n",
    "$$\\exp(x)\\exp(x) \\ne \\left(\\sum_{n=0}^\\infty\\frac{x^n}{n!}\\right)\\;\\left(\\sum_{n=0}^\\infty\\frac{y^n}{n!}\\right). \\;\\;(\\text{Nonsense expression!})$$<br/>\n",
    "Next, we make use of the fact that the product of two series can be written as a double sum:<br/>\n",
    "$$\\exp(x)\\exp(x) = \\sum_{n=0}^\\infty \\sum_{m=0}^\\infty \\frac{x^n}{n!} \\frac{y^m}{m!}.$$<br/>\n",
    "Here, we are summing over all possible pair-wise combinations of $n$ and $m$, which is precisely what happens when one expands the product of two series according to the usual rules of algebra.  The next step is to perform a *change of variables* on $m$ and $n$. In the above expression, we are summing over all non-negative integer $m$ and $n$; however, the bound variable $n$ can be re-expressed in terms of a newly-defined variable,<br/>\n",
    "$$N = m + n.$$</br>\n",
    "In the original double sum, $n$ and $m$ both run from $0$ to $+\\infty$, so it follows that their sum $N$ runs from $0$ to $+\\infty$. For each given value of $N$, we can write $n = N - m$, and moreover the allowed values of $m$ would only go from $0$ to $N$ (it can't exceed $N$, otherwise $n$ would be negative). In this way, the double sum is converted to<br/>\n",
    "$$\\exp(x)\\exp(x) = \\sum_{N=0}^\\infty \\sum_{m=0}^N \\frac{x^{N-m}}{(N-m)!} \\frac{y^m}{m!}$$<br/>\n",
    "Note that after this change of variables, the two summation signs are no longer interchangeable.  In the $\\sum_{m=0}^N$ sign, the variable $N$ appears in the upper limit, so this needs to be written to the right of $\\sum_{N=0}^\\infty$.  One sum is thus \"encapsulated\" inside the other; we could write the algebraic expression more rigorously like this:\n",
    "$$\\exp(x)\\exp(x) = \\sum_{N=0}^\\infty \\left(\\sum_{m=0}^N \\frac{x^{N-m}}{(N-m)!} \\frac{y^m}{m!}\\right).$$<br/>\n",
    "Finally, we use the <a href=\"https://en.wikipedia.org/wiki/Binomial_theorem\">binomial theorem</a> to simplify the inner sum:<br/>\n",
    "$$\\exp(x)\\exp(x) = \\sum_{N=0}^\\infty \\frac{\\left(x + y\\right)^N}{N!}, \\;\\;\\;\\text{since} \\;\\; (x+y)^N = \\sum_{m=0}^N \\frac{N!}{m!(N-m)!} x^{N-m} \\, y^m.$$<br/>\n",
    "Referring again to the series definition of the exponential, we obtain the desired result:<br/>\n",
    "$$\\exp(x)\\exp(x) = \\exp(x+y)$$<br/>\n",
    "</li>\n",
    "<li value=\"4\"><a name=\"01_04\"></a>\n",
    "The <a href=\"01_mathfunctions.ipynb#powers\">definition</a> of non-natural powers is<br/>\n",
    "$$a^b = \\exp[b\\ln(a)].$$\n",
    "Let $a = \\exp(1) = e$ and $b = x$. Then<br/>\n",
    "$$[\\exp(1)]^x = \\exp\\left[x\\ln\\Big(\\exp(1)\\Big)\\right].$$\n",
    "Since the logarithm is the inverse of the exponential function, $\\ln(\\exp(1)) = 1$. Hence,\n",
    "$$e^x = \\exp(x).$$<br/>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Derivatives\n",
    "\n",
    "<ol>\n",
    "<li value=\"2\"><a name=\"02_02\"></a>\n",
    "If $y = \\ln(x)$, it follows from the definition of the logarithm that\n",
    "\n",
    "$$\\exp(y) = x.$$\n",
    "\n",
    "Taking $d/dx$ on both sides, and using the [product rule](01_derivatives.ipynb#composition_rules), gives\n",
    "\n",
    "$$\\frac{dy}{dx} \\, \\exp(y) = 1 \\;\\;\\; \\Rightarrow \\frac{dy}{dx} = \\frac{1}{\\exp(y)} = \\frac{1}{x}.$$<br/>\n",
    "</li>\n",
    "<li value=\"8\"><a name=\"02_08\"></a>\n",
    "For an ordinary differential equation for a scalar (one-component) function of order $n$, the general solution must contain $n$ independent variables. In this case, $\\vec{v}$ is a two-component function, so it requires $2n$ indpendent variables. The differential equation<br/>\n",
    "$$\\frac{d\\vec{v}}{dx} = \\mathbf{A} \\vec{v}$$<br/>\n",
    "has order $n = 1$, so a total of 2 independent variables is required for the general solution.\n",
    "\n",
    "Let $u$ be an eigenvector of $\\mathbf{A}$ with eigenvalue $\\lambda$, and suppose that $\\vec{v}(x) = \\vec{u}\\,e^{\\lambda x}$ (note that $\\vec{u}$ itself does not depend on $x$).  Then<br/>\n",
    "$$\\begin{aligned}\\frac{d\\vec{v}}{dx} &= \\vec{u} \\frac{d}{dx}\\left(e^{\\lambda x}\\right) \\\\\n",
    "&= \\lambda \\, \\vec{u}\\, e^{\\lambda x} \\\\\n",
    "&= \\left(\\mathbf{A} \\vec{u}\\right) e^{\\lambda x} \\\\\n",
    "&= \\mathbf{A} \\left(\\vec{u} e^{\\lambda x}\\right) \\\\\n",
    "&= \\mathbf{A} \\vec{v}(x).\n",
    "\\end{aligned}$$<br/>\n",
    "Hence, $\\vec{v}(x)$ satisfies the desired differential equation.\n",
    "\n",
    "Let $\\vec{u}_1$ and $\\vec{u}_2$ be the eigenvectors of $\\mathbf{A}$, with eigenvalues $\\lambda_1$ and $\\lambda_2$.  The general solutions will be<br/>\n",
    "$$\\vec{v}(x) = c_1 \\vec{u}_1 e^{\\lambda_1 x} + c_2 \\vec{u}_2 e^{\\lambda_2 x},$$<br/>\n",
    "where $c_1$ and $c_2$ are independent variables.\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrals\n",
    "\n",
    "<ol>\n",
    "<li value=\"4\"><a name=\"03_04\"></a>\n",
    "Let us define<br/>\n",
    "$$I(\\gamma) = \\int_0^1 \\frac{x^\\gamma - 1}{\\ln(x)},$$<br/>\n",
    "so that $I(2)$ is our desired integral.  To take the derivative, first note that<br/>\n",
    "$$\\frac{d}{d\\gamma}(x^\\gamma) = \\ln(x)\\, x^\\gamma,$$<br/>\n",
    "which can be proven using the <a href=\"01_mathfunctions.ipynb#powers\">generalized definition of the power operation</a>.  Thus,<br/>\n",
    "$$\\begin{aligned}\\frac{d}{d\\gamma} I(\\gamma) &= \\int_0^1 \\frac{\\ln(x) x^\\gamma}{\\ln(x)} \\\\\n",
    "&= \\int_0^1 x^\\gamma \\\\\n",
    "&= \\frac{1}{1+\\gamma}.\\end{aligned}$$<br/>\n",
    "This can be integrated straightforwardly:<br/>\n",
    "$$I(\\gamma) = \\int \\frac{d\\gamma}{1+\\gamma} = \\ln(1+\\gamma) + c,$$\n",
    "where $c$ is a constant of integration, which we now have to determine.  Referring to the original definition of $I(\\gamma)$, observe that $I(0) = \\int_0^1 (1 - 1)/\\ln(x) = 0$.  This implies that $c = 0$.  Therefore, the answer is<br/>\n",
    "$$I(2) = \\ln(3).$$\n",
    "</li>\n",
    "<li value=\"6\"><a name=\"03_06\"></a>\n",
    "We are provided with the following ansatz for the solution to the differential equation:<br/>\n",
    "$$y(t) = y(0) + \\int_0^t dt' e^{-\\gamma(t-t')} g(t').$$<br/>\n",
    "First, note that when $t = 0$, the integral's range shrinks to zero, so the result is $y(0)$, as expected.  In order to determine the appropriate function $g$, we perform a derivative in $t$. The tricky part is that $t$ appears in two places: in the upper range of the integral, as well as in the integrand. So when we take the derivative, there should be two distinct terms (see <a href=\"03_integrals.ipynb#exercises\">problem 5</a>):<br/>\n",
    "$$\\begin{aligned}\\frac{dy}{dt} &= \\left[e^{-\\gamma(t-t')} g(t')\\right]_{t'=t} + \\int_0^t dt'(-\\gamma) \\, e^{-\\gamma(t-t')} \\, g(t')\\\\\n",
    "&= g(t) - \\gamma [y(t) - y(0)].\\end{aligned}$$<br/>\n",
    "In the last step, we again made use of the ansatz for $y(t)$.  Finally, comparing this with the original differential equation for $y(t)$, we find that<br/>\n",
    "$$g(t) - \\gamma [y(t) - y(0)] = -\\gamma y(t) + f(t) \\;\\;\\; \\Rightarrow \\;\\;\\; g(t) = f(t) - \\gamma y(0).$$<br/>\n",
    "Hence, the solution to the differential equation is<br/>\n",
    "$$\\begin{aligned}y(t) &= y(0) + \\int_0^t dt' \\, e^{-\\gamma(t-t')} \\,[f(t') - \\gamma y(0)] \\\\\n",
    "&= y(0)\\,e^{-\\gamma t} + \\int_0^t dt' \\, e^{-\\gamma(t-t')} f(t').\\end{aligned}$$<br/>\n",
    "</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Complex Numbers\n",
    "\n",
    "<ol>\n",
    "<li value=\"2\"><a name=\"04_02\"></a>\n",
    "Using the polar representation: let $z_1 = r_1 \\exp(i\\theta_1)$ and $z_2 = r_2 \\exp(i\\theta_2)$. Then\n",
    "$$\\begin{aligned}\\big|z_1\\, z_2\\big| &= \\left|r_1 e^{i\\theta_1} \\, r_2 e^{i\\theta_2}\\right| \\\\&= \\left|(r_1r_2)\\,e^{i(\\theta_1+\\theta_2)}\\right| \\\\&= r_1 r_2 \\\\&= |z_1|\\, |z_2|.\\end{aligned}$$\n",
    "Using the Cartesian representation: let $z_1 = x_1+iy_1$ and $z_2 = x_2+iy_2$.  For convenience, we evaluate the squared magnitude:\n",
    "$$\\begin{aligned}\\big|z_1\\, z_2\\big|^2 &= \\left|(x_1x_2 - y_1y_2) + i (x_1y_2+x_2y_1)\\right|^2 \\\\&= (x_1x_2 - y_1y_2)^2 + (x_1y_2+x_2y_1)^2 \\\\&= x_1^2x_2^2 + y_1^2 y_2^2 + x_1^2y_2^2 + x_2^2 y_1^2 \\\\&= \\left(x_1^2 + y_1^2\\right)\\left(x_2^2 + y_2^2\\right) \\\\&= |z_1|^2 \\, |z_2|^2.\\end{aligned}$$\n",
    "</li>\n",
    "<li value=\"3\"><a name=\"04_03\"></a>\n",
    "Using the polar representation: let $z_1 = r_1 \\exp(i\\theta_1)$ and $z_2 = r_2 \\exp(i\\theta_2)$. Then\n",
    "$$\\begin{aligned}\\big(z_1\\, z_2\\big)^* &= \\left(\\left(r_1r_2\\right) e^{i(\\theta_1+\\theta_2)}\\right)^* \\\\ &= \\left(r_1r_2\\right) e^{-i(\\theta_1+\\theta_2)} \\\\&= \\left(r_1 e^{-i\\theta_1}\\right) \\left(r_2 e^{-i\\theta_2}\\right) \\\\&= z_1^* \\, z_2^*.\\end{aligned}$$\n",
    "Using the Cartesian representation: let $z_1 = x_1+iy_1$ and $z_2 = x_2+iy_2$.\n",
    "$$\\begin{aligned}\\big(z_1\\, z_2\\big)^* &= \\left[\\left(x_1x_2 - y_1y_2\\right) + i \\left(x_1y_2+x_2y_1\\right)\\right]^* \\\\&= \\left(x_1x_2 - y_1y_2\\right) - i \\left(x_1y_2+x_2y_1\\right) \\\\&= \\left(x_1 - iy_1\\right) \\left(x_2 - iy_y\\right) \\\\&= z_1^* \\, z_2^*.\\end{aligned}$$\n",
    "</li>\n",
    "<li value=\"4\"><a name=\"04_04\"></a>\n",
    "The problem arises in this part of the chain: $i\\cdot i = \\sqrt{-1}\\,\\sqrt{-1} = \\sqrt{(-1)(-1)}$.  The square root is a non-integer power, and non-integer powers  are <a href=\"04_complex_numbers.ipynb#algebra\">not allowed to take part in standard complex algebra equations</a> in the same way as addition, subtraction, multiplication, division, and integer powers.<p>As discussed in <a href=\"08_branch_cuts.ipynb\">Chapter 7</a>, square roots and other non-integer powers have multiple values. The definition of the imaginary unit is often written as $i = \\sqrt{-1}$, but this is misleading. Actually, $\\sqrt{-1}$ has two legitimate values; one of these values is (by definition) $i$, while the other value is $-i$.</p></li>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Complex Oscillations\n",
    "\n",
    "<ol>\n",
    "<li value=\"4\"><a name=\"05_04\"></a>\n",
    "The general solution is $$z(t) = A \\exp\\left[-i(\\omega_1 - i \\gamma) t\\right].$$ It can be verified by direct substitution that this is a solution to the differential equation. It contains one free parameter, and the differential equation is first-order, so it must be a general solution. Next,$$\\begin{align}\\frac{d^2z}{dt^2} + 2 \\gamma \\frac{dz}{dt} &= (-i)^2(\\omega_1 - i\\gamma)^2 z(t) - 2i \\gamma (\\omega_1 - i \\gamma) z(t) \\\\ &= \\left[- \\omega_1^2 + \\gamma^2 + 2i\\gamma\\omega_1 - 2i \\gamma \\omega_1 - 2\\gamma^2)\\right] z(t) \\\\ &= -\\left(\\omega_1^2 + \\gamma^2\\right)z(t).\\end{align}$$ Hence, $z(t)$ obeys a damped harmonic oscillator equation with $\\omega_0^2 = \\omega_1^2 + \\gamma^2.$  This expression for the natural frequency ensures that $\\omega_0^2 > \\gamma^2$ (assuming the parameters $\\gamma$ and $\\omega_1$ are both real); hence, the harmonic oscillator is always under-damped.\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Complex Waves\n",
    "\n",
    "<ol>\n",
    "<li value=\"2\"><a name=\"06_02\"></a>\n",
    "Writing $n = n' + i n''$, where $n'$ and $n''$ are real, the travelling wave solutions are $$\\psi(x) = A \\exp\\left[\\pm i (n'+in'')\\frac{\\omega}{c} x\\right].$$ The magnitude and argument are: $$\\begin{align}\\big|\\psi(x)\\big| &= |A| \\exp\\left[\\mp n'' \\frac{\\omega}{c} x \\right] \\\\ \\mathrm{arg}\\big[\\psi(x)\\big] &= \\mathrm{arg}(A) \\pm n' \\frac{\\omega}{c} x.\\end{align}$$ The wave's propagation direction is determined by the argument: if the argument increases with $x$ then it is right-moving, and if the argument decreases with $x$ it is left-moving. Moreover, the wave is said to experience amplification if its amplitude grows along the propagation direction, and damping if its amplitude decreases along the propagation direction.<p>Consider the upper choice of sign (i.e., $+$ for the $\\pm$ symbol and $-$ for the $\\mp$ symbol). From the magnitude, we see that the wave's amplitude decreases with $x$ if $n'' > 0$, and increases with $x$ if $n'' < 0$. From the argument, the wave is right-moving if $n' >0$, and left-moving if $n' < 0$. Hence, the wave is damped if $n' n'' >0$ and amplified if $n' n'' < 0$.</p><p>(For example, consider the case $n' < 0$ and $n'' < 0$. The amplitude increases with $x$ but the wave is moving in the $-x$ direction; this means the amplitude grows in the direction opposite to the propagation direction, so the wave is damped.)</p><p>For the lower choice of sign, we see from the magnitdue that the amplitude increases with $x$ if $n'' > 0$, and decreases with $x$ if $n'' < 0$. From the argument, we see that the wave is left-moving if $n' >0$ and right-moving if $n' < 0$. Hence, the wave is damped if $n' n'' >0$ and amplified if $n' n'' < 0$, exactly the same as in the previous case.</p><p>Hence, whether the wave is amplified or damped only depends on the relative signs of $n'$ and $n''$, and is independent of the direction of propagation.</p>  \n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Complex Derivatives\n",
    "\n",
    "<ol>\n",
    "<li value=\"3\"><a name=\"07_03\"></a>\n",
    "We will use the Cauchy-Riemann equations.  Decompose $z$, $f$, and $g$ into real and imaginary parts as follows: $z = x + i y$, $f = u + i v$, and $g = p + i q$.  Since $f(z)$ and $g(z)$ are analytic in $D$, they satisfy $$\\begin{align} \\frac{\\partial u}{\\partial x} &= \\frac{\\partial v}{\\partial y},\\;\\; -\\frac{\\partial u}{\\partial y} = \\frac{\\partial v}{\\partial x}\\\\ \\frac{\\partial p}{\\partial x} &= \\frac{\\partial q}{\\partial y},\\;\\; -\\frac{\\partial p}{\\partial y} = \\frac{\\partial q}{\\partial x}.\\end{align}$$  This holds for all $z \\in D$. Next, expand the product $f(z)\\,g(z)$ into real and imaginary parts: $$\\begin{align} f(z)\\,g(z) = A(x,y) + i B(x,y),\\;\\;\\mathrm{where}\\;\\; \\begin{cases}A = up - v q \\\\ B = uq + vp. \\end{cases} \\end{align}$$ Our goal is to prove that $A$ and $B$ satisfy the Cauchy-Riemann equations for $x + i y \\in D$, which would then imply that $fg$ is analytic in $D$.  Using the product rule for derivatives: $$\\begin{align} \\frac{\\partial A}{\\partial x} &= \\frac{\\partial u}{\\partial x} p + u \\frac{\\partial p}{\\partial x} - \\frac{\\partial v}{\\partial x} q - v \\frac{\\partial q}{\\partial x} \\\\ &= \\frac{\\partial v}{\\partial y} p + u \\frac{\\partial q}{\\partial y} + \\frac{\\partial u}{\\partial y} q + v \\frac{\\partial p}{\\partial y} \\\\ \\frac{\\partial B}{\\partial y} &= \\frac{\\partial u}{\\partial y} q + u \\frac{\\partial q}{\\partial y} + \\frac{\\partial v}{\\partial y} p + v \\frac{\\partial p}{\\partial y}. \\end{align}$$ By direct comparison, we see that the two expressions are equal. Similarly, $$\\begin{align} \\frac{\\partial A}{\\partial y} &= \\frac{\\partial u}{\\partial y} p + u \\frac{\\partial p}{\\partial y} - \\frac{\\partial v}{\\partial y} q - v \\frac{\\partial q}{\\partial y} \\\\  &= - \\frac{\\partial v}{\\partial x} p - u \\frac{\\partial q}{\\partial x} - \\frac{\\partial u}{\\partial x} q - v \\frac{\\partial p}{\\partial x} \\\\ \\frac{\\partial B}{\\partial x} &=\n",
    "  \\frac{\\partial u}{\\partial x} q + u \\frac{\\partial q}{\\partial x} + \\frac{\\partial v}{\\partial x} p + v \\frac{\\partial p}{\\partial x}.\\end{align}$$ These two are the negatives of each other. Q.E.D.    \n",
    "</li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Branch Points and Branch Cuts\n",
    "\n",
    "<ol>\n",
    "<li value=\"1\"><a name=\"08_01\"></a>\n",
    "We can write $i$ in polar coordinates as $\\exp(i\\pi/2).$  Hence, $$\\begin{align}(i)^i &= \\exp\\Big\\{i \\ln\\big[\\exp(i\\pi/2)\\big]\\Big\\} \\\\ &= \\exp\\left\\{i \\left[\\frac{i\\pi}{2} + 2 \\pi i n\\right]\\right\\}, \\quad n \\in \\mathbb{Z} \\\\ &= \\exp\\left[- 2\\pi\\left(n+\\frac{1}{4}\\right) \\right], \\quad n \\in \\mathbb{Z}.\\end{align}$$\n",
    "</li>\n",
    "<li value=\"2\"><a name=\"08_02\"></a>\n",
    "Let $z = r\\exp(i\\theta)$, where $r > 0$.  The values of the logarithm are $$\\ln(z) = \\ln(r) + i (\\theta + 2\\pi n), \\;\\;\\;n \\in \\mathrm{Z}.$$  For each $n$, note that the first term is the real part and the second term is the imaginary part of a complex number $w_n$. The logarithm in the first term can be taken to be the real logarithm.<p>For $z \\rightarrow 0$, we have $r \\rightarrow 0$ and hence $\\ln(r)\\rightarrow -\\infty$. This implies that $w_n$ lies infinitely far to the left of the origin on the complex plane.  Therefore, $w_n \\rightarrow \\infty$ (referring to the <a href=\"08_branch_cuts.ipynb#infinity\">complex infinity</a>) regardless of the value of $n$. Likewise, for $z \\rightarrow \\infty$, we have $r \\rightarrow \\infty$ and hence $\\ln(r)\\rightarrow +\\infty$. This implies that $w_n$ lies infinitely far to the right of the origin on the complex plane, so $w_n \\rightarrow \\infty$ regardless of the value of $n$.  Therefore, $0$ and $\\infty$ are both branch points of the complex logarithm.</p>\n",
    "</li>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Contour Integration\n",
    "\n",
    "<ol>\n",
    "<li value=\"2\"><a name=\"09_02\"></a>\n",
    "By analytic continuation, consider the integral $$I = \\oint \\frac{dz}{z^4 + 1},$$ where the contour is closed in the upper half-plane (we could also choose to close below without changing the results). The contour integral over the large arc scales with the arc radius $R$ as $R^{-3}$, so it vanishes as $R \\rightarrow \\infty$. Hence, $I$ is exactly equal to the definite integral we are after.\n",
    "    \n",
    "To evaluate the loop integral, we need the poles of the integrand, which are the solutions to $z^4 = -1$. Writing $-1 = \\exp(i\\pi)$, we find that the roots are $\\exp(i\\pi/4) \\times \\{\\text{4-roots of unity}\\}.$ These can be written in the Cartesian representation as $$\\begin{align} z_1 &= \\frac{1+i}{\\sqrt{2}} \\\\ z_2 &= \\frac{-1+i}{\\sqrt{2}} \\\\ z_3 &= \\frac{-1-i}{\\sqrt{2}} \\\\ z_4 &= \\frac{1-i}{\\sqrt{2}}.\\end{align}$$ By closing the contour above, we enclose $z_1$ and $z_2$. Thus, by the residue theorem, $$\\begin{align}I &= 2\\pi i \\left\\{\\left[\\mathrm{Res} \\left(\\frac{1}{z^4 + 1}\\right)\\right]_{z=z_1} + \\left[\\mathrm{Res} \\left(\\frac{1}{z^4 + 1}\\right)\\right]_{z=z_2}\\right\\} \\\\ &= 2\\pi i\\left[\\frac{1}{(z_1-z_2)(z_1-z_3)(z_1-z_4)} + \\frac{1}{(z_2-z_1)(z_2-z_3)(z_2-z_4)}\\right] \\\\ &= 2\\pi i \\left[\\frac{\\sqrt{8}}{(2)(2+2i)(2i)} + \\frac{\\sqrt{8}}{(-2)(2i)(-2+2i)}\\right] \\\\ &= \\frac{\\sqrt{2}\\pi i}{2} \\left[\\frac{1}{-1+i} + \\frac{1}{1+i}\\right] \\\\ &= \\frac{\\pi}{\\sqrt{2}}.\\end{align}$$\n",
    "</li>\n",
    "<li value=\"6\"><a name=\"09_06\"></a>\n",
    "A unit circle centered at the origin can be parameterized by $z = \\exp(i\\phi)$.  Hence, along this circle, $$\\begin{align}\\cos\\phi &= \\frac{1}{2}\\left(e^{i\\phi} + e^{-i\\phi}\\right) \\\\ &= \\frac{1}{2} \\left(e^{i\\phi} + \\frac{1}{e^{i\\phi}}\\right) \\\\ &= \\frac{1}{2}\\left(z + \\frac{1}{z}\\right).\\end{align}$$ Also, $$\\frac{dz}{d\\phi} = iz.$$ Let us denote this circular contour by $\\Gamma$. We want to find a function $f(z)$ such that $$\\oint_\\Gamma f(z) \\;dz = \\int_0^{2\\pi} \\frac{d\\phi}{\\cos\\phi+3}.$$ The contour integral on the left side can be parameterized as $$\\int_0^{2\\pi} d\\phi\\; f\\big(z(\\phi)\\big)\\, \\frac{dz}{d\\phi}.$$ Therefore, we want $f(z)$ such that $$\\begin{align}f\\big(z(\\phi)\\big) \\frac{dz}{d\\phi} &= \\frac{1}{\\cos\\phi+3} \\\\ = f(z) \\;\\,(i z) &= \\frac{1}{\\frac{1}{2}\\left(z+\\frac{1}{z}\\right) + 3}.\\end{align}$$ After some algebra, we obtain $$f(z) = \\frac{-2i}{z^2 + 6z + 1}.$$ The denominator in $f(z)$ has two roots, which are both real: $$\\begin{align}z_+ &= -3 + 2\\sqrt{2} = - 0.17157\\dots \\\\ z_- &= -3 - 2\\sqrt{2} = -5.8284\\dots\\end{align}$$ Only the $z_+$ pole is enclosed by the unit circle. Thus, we can use the residue theorem to evaluate the integral: $$\\begin{align}I = \\oint_\\Gamma \\frac{-2i}{z^2 + 6z + 1} \\, dz &= 2\\pi i \\; \\mathrm{Res}\\left[\\frac{-2i}{(z-z_+)(z-z_-)}\\right]_{z = z_+} \\\\ &= 2\\pi i \\left(\\frac{-2i}{z_+-z_-}\\right) \\\\ &= \\frac{4\\pi}{\\left(-3 + 2\\sqrt{2}\\right)-\\left(-3 - 2\\sqrt{2}\\right)} \\\\ &= \\frac{\\pi}{\\sqrt{2}}.\\end{align}$$\n",
    "</li>\n",
    "<li value=\"7\"><a name=\"09_07\"></a>\n",
    "To evaluate the principal-value integral $$I = \\mathcal{P}\\left[\\int_{-\\infty}^\\infty \\frac{f(x)}{x-a} dx \\right],$$ we introduce the following loop contour:\n",
    "<img src=\"img/principal_value_integral2.png\" alt=\"Contour used to evaluate a principal value integral.\" />\n",
    "The solution procedure is very similar to the <a href=\"09_contour_integration.ipynb#principal-value\">example worked out in the section of principal-value integrals</a>. From the properties of $f(z)$ given in the problem statement, we can conclude that (i) the integrand is analytic on and within the loop contour, so the residue theorem can be used; and (ii) the integrand vanishes quickly enough far from the origin so that, by Jordan's lemma, the integral over $\\Gamma_2$ vanishes. Hence, $$I = i \\pi f(a).$$ By relabelling the dummy variables $x \\rightarrow y$ and $a \\rightarrow x$, we can write $$f(x) = -\\frac{i}{\\pi} \\mathcal{P}\\left[\\int_{-\\infty}^\\infty \\frac{f(y)}{y-x} dy \\right].$$ Let us now break up $f$ into its real and imaginary parts: $$\\begin{align}\\mathrm{Re}[f(x)] + i \\mathrm{Im}[f(x)] &= -\\frac{i}{\\pi} \\mathcal{P}\\left[\\int_{-\\infty}^\\infty \\frac{\\mathrm{Re}[f(y)] + i \\mathrm{Im}[f(y)]}{y-x} \\;dy \\right] \\\\ &= \\frac{1}{\\pi} \\mathcal{P}\\left[\\int_{-\\infty}^\\infty \\frac{\\mathrm{Im}[f(y)] -i\\mathrm{Re}[f(y)]}{y-x} \\;dy \\right].\\end{align}$$ Equating the real and imaginary parts of the two sides, we obtain the following two real equations, which are the Kramers-Kronig relations: $$\\begin{align}\\mathrm{Re}[f(x)] &= \\;\\;\\,\\frac{1}{\\pi} \\mathcal{P}\\left[\\int_{-\\infty}^\\infty \\frac{\\mathrm{Im}[f(y)]}{y-x} \\;dy \\right] \\\\ \\mathrm{Im}[f(x)] &= -\\frac{1}{\\pi} \\mathcal{P}\\left[\\int_{-\\infty}^\\infty \\frac{\\mathrm{Re}[f(y)]}{y-x} \\; dy \\right].\\end{align}$$\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Fourier Series and Fourier Transforms\n",
    "\n",
    "<ol>\n",
    "<li value=\"3\"><a name=\"10_03\"></a>\n",
    "The Fourier coefficients are given by $$f_n = \\frac{1}{a} \\int_{-a/2}^{\\,a/2} dx\\; e^{-i k_n x}\\, f(x), \\quad \\mathrm{where}\\;\\, k_n = \\frac{2\\pi n}{a}.$$ First, consider the case where $f(x)$ is real.  Take the complex conjugate of both sides: $$\\begin{align}f_n^* &= \\frac{1}{a} \\int_{-a/2}^{\\,a/2} dx\\; \\left(e^{-i k_n x}\\, f(x)\\right)^* \\\\ &= \\frac{1}{a} \\int_{-a/2}^{\\,a/2} dx\\; e^{i k_n x}\\, f(x)^* \\\\ &= \\frac{1}{a} \\int_{-a/2}^{\\,a/2} dx\\; e^{i k_n x}\\, f(x) \\\\ &= f_{-n}.\\end{align}$$ Hence, $$f_{n} = f_{-n}^*.$$ For the second case, $f(x) = f(-x),$ perform a change of variables $x = -u$ in the Fourier integral: $$\\begin{align}f_n &= \\frac{1}{a} \\int_{-a/2}^{\\,a/2} du\\; e^{i k_n u}\\, f(u) \\\\ &= f_{-n}.\\end{align}$$ For $f(x) = f(-x)^*$, the same change of variables gives $$f_n = f_n^*.$$\n",
    "</li>\n",
    "<li value=\"7\"><a name=\"10_07\"></a>\n",
    "From the <a href=\"10_fourier_transform.ipynb#delta_function\">definition of the delta function</a> as the narrow-peak limit of a Gaussian wavepacket: $$\\delta(ax) = \\lim_{\\gamma \\rightarrow 0} \\, \\int_{-\\infty}^\\infty \\frac{dk}{2\\pi} \\, e^{ikax} \\, e^{-\\gamma k^2}.$$ Perform a change of variables $k = q/a$ and $\\gamma = \\gamma' \\, a^2$: $$\\begin{align}\\delta(ax) &= \\lim_{\\gamma' \\rightarrow 0} \\, \\int_{-\\infty}^\\infty \\frac{1}{a}\\frac{dq}{2\\pi} \\, e^{iqx} \\, e^{-\\gamma' q^2} \\\\ &= \\frac{1}{a} \\delta(x).\\end{align}$$\n",
    "</li>\n",
    "<li value=\"8\"><a name=\"10_08\"></a>\n",
    "Perform a change of variables from Cartesian coordinates $(x,y)$ to polar coordinates $(r,\\phi)$: $$\\begin{align} \\int_{-\\infty}^\\infty dx \\int_{-\\infty}^\\infty dy \\; x^2\\, \\delta\\left(\\sqrt{x^2+y^2}-a\\right) &= \\int_0^{\\infty} dr \\int_{0}^{2\\pi} rd\\phi\\, \\cdot\\, r^2\\cos^2\\phi\\; \\delta(r-a) \\\\&= \\left(\\int_0^{\\infty} dr \\, r^3\\, \\delta(r-a)\\right) \\left(\\int_{0}^{2\\pi}\\!d\\phi \\, \\cos^2\\phi\\right) \\\\ &= \\begin{cases}\\pi a^3, & a \\ge 0 \\\\ 0, & a < 0.\\end{cases} \\end{align}$$\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Green's Functions\n",
    "\n",
    "<ol>\n",
    "<li value=\"2\"><a name=\"11_02\"></a>\n",
    "For the over-damped oscillator, the Green's function is $$G(t,t') = \\Theta(t-t')\\, \\frac{e^{-\\gamma(t-t')}}{\\Gamma} \\sinh\\big[\\Gamma(t-t')\\big], \\quad\\mathrm{where}\\;\\,\\Gamma = \\sqrt{\\gamma^2 - \\omega_0^2}.$$ Hence, the response to the force $f$ is $$x(t) = \\frac{1}{m\\Gamma} \\int^t_{-\\infty} dt'\\; e^{-\\gamma(t-t')} \\sinh\\big[\\Gamma(t-t')\\big] f(t').$$ From this, we get the following expression for the desired correlation function: $$\\langle x(t_1)\\, x(t_2)\\rangle = \\frac{1}{m^2\\Gamma^2} \\int^{t_1}_{-\\infty} dt' \\int^{t_2}_{-\\infty} dt'' \\; e^{-\\gamma(t_1-t')}\\, e^{-\\gamma(t_2-t'')} \\sinh\\big[\\Gamma(t_1-t')\\big] \\sinh\\big[\\Gamma(t_2-t'')\\big] \\; \\langle f(t') f(t'')\\rangle.$$ Note that the $\\langle\\cdots\\rangle$ can be shifted inside the integrals, because it represents taking the mean over independent sample trajectories. Now, without loss of generality, let us take $$t_1 \\ge t_2.$$  Since $\\langle f(t') f(t'')\\rangle = A \\delta(t'-t'')$ which vanishes for $t' \\ne t''$, the double integral only receives contributions from values of $t'$ not exceeding $t_2$ (which is the upper limit of the range for $t''$). Thus, we revise $\\int^{t_1} dt'$ into $\\int^{t_2} dt'$. The delta function then reduces the double integral into a single integral, which can be solved and simplified with a bit of tedious algebra: $$\\begin{align}\\langle x(t_1)\\, x(t_2)\\rangle &= \\frac{A}{m^2\\Gamma^2} e^{-\\gamma(t_1+t_2)} \\int^{t_2}_{-\\infty} dt' e^{2\\gamma t'} \\sinh\\big[\\Gamma(t'-t_1)\\big] \\, \\sinh\\big[\\Gamma(t'-t_2)\\big] \\\\ &= \\frac{A}{8m^2\\Gamma^2} e^{-\\gamma(t_1+t_2)}\\left[\\frac{e^{-\\Gamma t_1} e^{(2\\gamma+\\Gamma)t_2}}{\\gamma+\\Gamma} + \\frac{e^{\\Gamma t_1}e^{(2\\gamma-\\Gamma)t_2}}{\\gamma-\\Gamma} - \\frac{e^{-\\Gamma t_1} e^{(\\Gamma+2\\gamma)t_2} + e^{\\Gamma t_1} e^{(-\\Gamma+2\\gamma)t_2}}{\\gamma}\\right] \\\\ &= \\frac{A}{8m^2\\Gamma\\gamma} \\left[\\frac{e^{-(\\gamma-\\Gamma)(t_1-t_2)}}{\\gamma-\\Gamma} - \\frac{e^{-(\\gamma+\\Gamma)(t_1-t_2)}}{\\gamma+\\Gamma} \\right].\\end{align}$$ Hence, $$\\begin{align}\\left\\langle [x(t+\\Delta t) - x(t)]^2\\right\\rangle &= 2\\Big[\\left\\langle x(t)^2\\right\\rangle - \\left\\langle x(t+\\Delta t) x(t)\\right\\rangle\\Big] \\\\ &= \\frac{A}{4m^2\\Gamma\\gamma} \\left[\\frac{1-e^{-(\\gamma-\\Gamma)\\Delta t}}{\\gamma-\\Gamma} - \\frac{1-e^{-(\\gamma+\\Gamma)\\Delta t}}{\\gamma+\\Gamma} \\right].\\end{align}$$\n",
    "</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
